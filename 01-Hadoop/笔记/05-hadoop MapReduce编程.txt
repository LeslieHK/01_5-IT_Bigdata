1、Map
public class WordCountMap extends Mapper<LongWritable, Text, Text, LongWritable>{
	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		String line = value.toString();
		
		for(String word : line.split(" ")){
			context.write(new Text(word), new LongWritable(1));
		}
	}
}

2、Reduce
public class WordCountReduct extends Reducer<Text, LongWritable, Text, LongWritable>{

	@Override
	protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
		int count = 0;
		for(LongWritable value : values){
			count += value.get();
		}
		context.write(key, new LongWritable(count));	
	}	
}

3、Job
public class WordCountRunner {
	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		Job wcJob = Job.getInstance(conf);
		
		conf.set("mapreduct.job.jar", "WordCount.jar");
		wcJob.setJarByClass(WordCountRunner.class);
		
		wcJob.setMapperClass(WordCountMap.class);
		wcJob.setReducerClass(WordCountReduct.class);
		
		wcJob.setMapOutputKeyClass(Text.class);
		wcJob.setMapOutputValueClass(LongWritable.class);
		
		wcJob.setOutputKeyClass(Text.class);
		wcJob.setOutputValueClass(LongWritable.class);
		
		FileInputFormat.setInputPaths(wcJob, "hdfs://programmer:9000/tmp");
		FileOutputFormat.setOutputPath(wcJob, new Path("hdfs://programmer:9000/tmp/result"));
		
		wcJob.waitForCompletion(true);
	}

}

4、执行MR的命令：
	hadoop jar <jar在linux的路径> <main方法所在的类的全类名> <参数>
	例子：
	hadoop jar WordCount.jar org.apache.hadoop.wordcount.WordCountRunner

5、MR执行流程
	(1).客户端提交一个mr的jar包给JobClient(提交方式：hadoop jar ...)
	(2).JobClient通过RPC和JobTracker进行通信，返回一个存放jar包的地址（HDFS）和jobId
	(3).client将jar包写入到HDFS当中(path = hdfs上的地址 + jobId)
	(4).开始提交任务(任务的描述信息，不是jar, 包括jobid，jar存放的位置，配置信息等等)
	(5).JobTracker进行初始化任务
	(6).读取HDFS上的要处理的文件，开始计算输入分片，每一个分片对应一个MapperTask
	(7).TaskTracker通过心跳机制领取任务（任务的描述信息）
	(8).下载所需的jar，配置文件等
	(9).TaskTracker启动一个java child子进程，用来执行具体的任务（MapperTask或ReducerTask）
	(10).将结果写入到HDFS当中
	
	